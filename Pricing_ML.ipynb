{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71805ba4-9814-4cc8-919b-22b31de215b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    
    "!pip install matplotlib\n",
    "!pip install networkx\n",
    "!pip install numpy\n",
    "!pip install haversine\n",
    "!pip install cartopy\n",
    "!pip install geopandas\n",
    "!pip install datetime\n",
    "!pip install openpyxl\n",
    "!pip install ortools \n",
    "!pip install pyro-ppl \n",
    "!pip install fiona shapely pyproj rtree\n",
    "!pip install --upgrade fiona geopandas\n",
    "   \n",
    "import os\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ortools\n",
    "from ortools.linear_solver import pywraplp\n",
    "from haversine import haversine\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from openpyxl import load_workbook\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from openpyxl import load_workbook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyro.infer import Predictive\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "# copy the repository\n",
    "\n",
    "repo_url = \"https://github.com/rimchmielowitz/Pricing_ML.git\"\n",
    "repo_name = \"Pricing_ML\"\n",
    "\n",
    "if not os.path.exists(repo_name):\n",
    "    !git clone --recurse-submodules {repo_url}\n",
    "    print(\"✅ Repository wurde geklont.\")\n",
    "else:\n",
    "    print(\"✅ Repository existiert bereits.\")\n",
    "\n",
    "# change to repository folder\n",
    "os.chdir(repo_name)\n",
    "\n",
    "# SHAPE_RESTORE_SHX aktivieren\n",
    "os.environ[\"SHAPE_RESTORE_SHX\"] = \"YES\"\n",
    "\n",
    "#paths of Berlin instances\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), \"data_Berlin\")\n",
    "data1 = os.path.join(data_folder, \"TWD.csv\")\n",
    "data2 = os.path.join(data_folder, \"districts.csv\")\n",
    "\n",
    "M = 10000\n",
    "time_format_output='%H:%M'\n",
    "\n",
    "#model\n",
    "def pricing_model(n, m, A, A_, H, WTP, ETP, R, d2, V, d, t, s, l, a, b, M):\n",
    "    \n",
    "    # Initializing OR-Tools Solver (MIP-Optimization)\n",
    "    solver = pywraplp.Solver.CreateSolver('SCIP')\n",
    "\n",
    "    if not solver:\n",
    "        print(\"Solver konnte nicht erstellt werden!\")\n",
    "        return None\n",
    "\n",
    "    # Definitiion of variables\n",
    "    x = {}\n",
    "    for (i, j) in E:\n",
    "        for h in H:\n",
    "            x[i, j, h] = solver.IntVar(0, 1, f'x[{i},{j},{h}]')\n",
    "\n",
    "    S = {}\n",
    "    for i in V:\n",
    "        for h in H:\n",
    "            S[i, h] = solver.IntVar(0, solver.infinity(), f'S[{i},{h}]')\n",
    "\n",
    "    L = {}\n",
    "    for i in V:\n",
    "        for h in H:\n",
    "            L[i, h] = solver.IntVar(0, solver.infinity(), f'L[{i},{h}]')\n",
    "\n",
    "    z = {}\n",
    "    for i in A:\n",
    "        z[i] = solver.IntVar(0, 1, f'z[{i}]')\n",
    "\n",
    "    p = {}\n",
    "    for (i, j) in E:\n",
    "        for h in H:\n",
    "            p[i, j, h] = solver.NumVar(0, solver.infinity(), f'p[{i},{j},{h}]')\n",
    "\n",
    "    c = {}\n",
    "    for (i, j) in E:\n",
    "        for h in H:\n",
    "            c[i, j, h] = solver.NumVar(0, solver.infinity(), f'c[{i},{j},{h}]')\n",
    "\n",
    "    # objective function: Maximize the profit (price - costs)\n",
    "    solver.Maximize(\n",
    "        solver.Sum(p[i, j, h] * d2[i] - c[i, j, h] * d[i, j] for (i, j) in E for h in H if i != j)\n",
    "    )\n",
    "\n",
    "    # Constraints\n",
    "    for h in H:\n",
    "        for i in A:\n",
    "            for j in pickup_delivery:\n",
    "                if i != j:\n",
    "                    solver.Add(p[i, j, h] <= WTP[i] / d2[i] + (1 - x[i, j, h]) * M) #Price_not_greater_than_WTP(02)\n",
    "\n",
    "    for h in H:\n",
    "        for i in origin_pickup_delivery[h]:\n",
    "            for j in pickup_delivery:\n",
    "                if i != j:\n",
    "                    solver.Add(c[i, j, h] >= ETP[h] * x[i, j, h]) #Compensation_not_less_than_ETP(03)\n",
    "\n",
    "    for h in H:\n",
    "        for i in A:\n",
    "            for j in pickup_delivery:\n",
    "                if i != j:\n",
    "                    solver.Add(p[i, j, h] >= c[i, j, h]) #Price_bigger_Compensation(04)\n",
    "\n",
    "    for i in A:\n",
    "        solver.Add(solver.Sum(x[i, j, h] for j in pickup_delivery for h in H if i != j) + z[i] == 1) #i_in_requestbank_or_picked_up(05)\n",
    "\n",
    "    for h in H:\n",
    "        for i in A:\n",
    "            solver.Add(\n",
    "                solver.Sum(x[i, j, h] for j in Vh[h] if (i, j, h) in x and i != j) - solver.Sum(x[j, n+i, h] for j in Vh[h] if (j, n+i, h) in x and i != n+i) == 0\n",
    "            ) #matched_picked_delivered_same_courier(06) \n",
    "\n",
    "    for h in H:\n",
    "        solver.Add(solver.Sum(x[origin[h], j, h] for j in pickup_destination[h]) == 1) #courier_starts_origin_to_pickup_or_destination(07)\n",
    "\n",
    "    for h in H:\n",
    "        solver.Add(solver.Sum(x[i, tau_[h], h] for i in delivery_origin[h] if i != tau_[h]) == 1) #courier_arrives_destination_from_delivery_or_origin(08)\n",
    "\n",
    "    for j in pickup_delivery:\n",
    "        for h in H:\n",
    "            solver.Add(solver.Sum(x[i, j, h] for i in V if i != j) - solver.Sum(x[j, i, h] for i in V if i != j) == 0) #courier_arrives_leaves_droppoff(09)\n",
    "    \n",
    "    for (i, j) in E:\n",
    "        for h in H:\n",
    "            solver.Add(S[i, h] + s[i] + t[i, j] <= S[j, h] + (1 - x[i, j, h]) * M) #courier_follows_matched_paths(10)/(34)\n",
    "\n",
    "    for i in V:\n",
    "        for h in H:\n",
    "            solver.Add(S[i, h] >= a[i]) \n",
    "            solver.Add(S[i, h] <= b[i]) #pickup_deliver_timewindow(11)\n",
    "\n",
    "    for i in A:\n",
    "        for h in H:\n",
    "            solver.Add(S[i, h] <= S[n + i, h]) #pickup_before_delivery(12)\n",
    "\n",
    "    for (i, j) in E:\n",
    "        for h in H:\n",
    "            solver.Add(L[i, h] + l[j] <= L[j, h] + (1 - x[i, j, h]) * M) #capacity_for_next_loading(13)\n",
    "\n",
    "    for i in V:\n",
    "        for h in H:\n",
    "            solver.Add(L[i, h] <= R[h]) #capacity_constraint(14)\n",
    "\n",
    "    for h in H:\n",
    "        solver.Add(L[origin[h], h] == 0) #origin_destination_empty_load(15)(1)\n",
    "        solver.Add(L[destination[h], h] == 0) #origin_destination_empty_load(15)(2)\n",
    "\n",
    "    # Linearization Constraints\n",
    "    for (i, j) in E:\n",
    "        for h in H:\n",
    "            if i!=j:\n",
    "                solver.Add(p[i, j, h] <= M * x[i, j, h]) #big M pricing (23)\n",
    "    for (i, j) in E:\n",
    "        for h in H:            \n",
    "            solver.Add(c[i, j, h] <= M * x[i, j, h]) #big M compensation (24)\n",
    "   \n",
    "    for (i, j) in E:\n",
    "        for h in H:\n",
    "            if i != j:\n",
    "                solver.Add(p[i, j, h] <= M * x[i, j, h])  # Falls x=0, dann ist p=0\n",
    "                solver.Add(p[i, j, h] >= 0)  # p darf nicht negativ sein\n",
    "\n",
    "    for (i, j) in E:\n",
    "        for h in H:\n",
    "            if i != j:\n",
    "                solver.Add(c[i, j, h] <= M * x[i, j, h])  # Falls x=0, dann ist p=0\n",
    "                solver.Add(c[i, j, h] >= 0)  # p darf nicht negativ sein\n",
    "    \n",
    "    # additional constraints\n",
    "    for i in tau[1:]:\n",
    "        solver.Add(solver.Sum(x[i, j, h] for h in H for j in V if i != j) == 1) #additional_constr (25)\n",
    "    \n",
    "    for h in H:\n",
    "        solver.Add(solver.Sum(x[i, j, h] for i in tau_[1:] for h in H for j in V if i!= j) == 0) #additional_constr (26)\n",
    "\n",
    "    # solve model\n",
    "    status = solver.Solve()\n",
    "\n",
    "    if status == pywraplp.Solver.OPTIMAL:\n",
    "        optimal_x = {(i, j, h): x[i, j, h].solution_value() for (i, j) in E for h in H}\n",
    "        optimal_S = {(i, h): S[i, h].solution_value() for i in V for h in H}\n",
    "        optimal_L = {(i, h): L[i, h].solution_value() for i in V for h in H}\n",
    "        optimal_z = {i: z[i].solution_value() for i in A}\n",
    "        optimal_p = {(i, j, h): p[i, j, h].solution_value() for (i, j) in E for h in H}\n",
    "        optimal_c = {(i, j, h): c[i, j, h].solution_value() for (i, j) in E for h in H}\n",
    "\n",
    "        OF_p = solver.Objective().Value()\n",
    "        Runtime1 = solver.WallTime()\n",
    "        \n",
    "        return OF_p, optimal_L, optimal_S, optimal_c, optimal_p, optimal_x, optimal_z, Runtime1\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data from instance\n",
    "def read_data_create_sets(data1):\n",
    "    delimiter=';'\n",
    "    df=pd.read_csv(data1, delimiter=delimiter, header=0, parse_dates=['Time a', 'Time b'], date_format =\"%H:%M\") #, date_parser=pd.to_datetime\n",
    "    dfdistricts=pd.read_csv(data2, delimiter=delimiter, header=0)\n",
    "    m=int(df.at[0,\"m\"])\n",
    "    n=int(df.at[0,\"n\"])\n",
    "    if type(df.at[0,\"WTP_Faktor\"])==np.float64:\n",
    "        WTP_Faktor=df.at[0,\"WTP_Faktor\"]\n",
    "    if type(df.at[0,\"WTP_Faktor\"])== str:\n",
    "        WTP_Faktor=df.loc[0:0,\"WTP_Faktor\"].str.replace(',','.').astype(float)[0]\n",
    "    if type(df.at[0,\"ETP_Faktor\"])==np.float64:\n",
    "        ETP_Faktor=df.at[0,\"ETP_Faktor\"]\n",
    "    if type(df.at[0,\"ETP_Faktor\"])== str:\n",
    "        ETP_Faktor=df.loc[0:0,\"ETP_Faktor\"].str.replace(',','.').astype(float)[0]\n",
    "    Speed=int(df.at[0,\"Speed\"])\n",
    "    start_rowpu=0                                                        \n",
    "    start_rowdel= start_rowpu + n                                          \n",
    "    start_rowdriver= start_rowdel + n\n",
    "    A=df.loc[start_rowpu:start_rowdel-1,'i'].astype(int).tolist()\n",
    "    A_=df.loc[start_rowdel:start_rowdriver-1,'i'].astype(int).tolist()\n",
    "    AllX=df.loc[start_rowpu:start_rowdriver+2*m-1,'Long'].str.replace(',','.').astype(float).tolist()\n",
    "    AllX.insert(0,0.0)\n",
    "    AllY=df.loc[start_rowpu:start_rowdriver+2*m-1,'Lat'].str.replace(',','.').astype(float).tolist()\n",
    "    AllY.insert(0,0.0)\n",
    "    R2=df.loc[start_rowdriver:start_rowdriver+m-1,'R'].astype(int).tolist()\n",
    "    R2.insert(0,0)\n",
    "    district_names={idx: value for idx, value in enumerate (dfdistricts.loc[0:14,'Name'].astype(str), start=0)}\n",
    "    district_coordx=dfdistricts.loc[start_rowpu:14,'Lat'].str.replace(',','.').astype(float).tolist()\n",
    "    district_coordy=dfdistricts.loc[start_rowpu:14,'Long'].str.replace(',','.').astype(float).tolist()\n",
    "    l={idx: int(value) for idx, value in enumerate (df.loc[start_rowpu:start_rowdriver+2*m-1,'l'].astype(int), start=1)}\n",
    "    a={idx: int(value) for idx, value in enumerate (df.loc[start_rowpu:start_rowdriver+2*m-1,'a'].astype(int), start=1)}\n",
    "    b={idx: int(value) for idx, value in enumerate (df.loc[start_rowpu:start_rowdriver+2*m-1,'b'].astype(int), start=1)}\n",
    "    s={idx: int(value) for idx, value in enumerate (df.loc[start_rowpu:start_rowdriver+2*m-1,'s'].astype(int), start=1)}\n",
    "    H = list(range(1, m + 1)) \n",
    "    TW_LB = {idx: pd.to_datetime(value, format=\"%H:%M:%S\").strftime(time_format_output) \n",
    "         for idx, value in enumerate(df.loc[start_rowpu:start_rowdriver+2*m-1, 'Time a'], start=1)}\n",
    "\n",
    "    TW_UB = {idx: pd.to_datetime(value, format=\"%H:%M:%S\").strftime(time_format_output) \n",
    "         for idx, value in enumerate(df.loc[start_rowpu:start_rowdriver+2*m-1, 'Time b'], start=1)}\n",
    "    R={h:R2[h] for h in H}\n",
    "    tau=[2*n+h for h in H]                                                     # origin nodes of couriers\n",
    "    tau.insert(0,0)\n",
    "    tau_=[2*n + m +h for h in H]                                               # destination nodes of couriers\n",
    "    tau_.insert(0,0)\n",
    "    ZF=[b[i+m]-a[i] for i in tau[1:]]\n",
    "    V= A + A_ +tau[1:] + tau_[1:]     \n",
    "    pickup_delivery= A+A_                                                      # all pickup and delivery nodes\n",
    "    destination={(h): tau_[h] for h in H}                                      # Set of destinations in dependence of driver h\n",
    "    origin={(h): tau[h] for h in H}                                            # Set of origins in dependence of driver h\n",
    "    pickup_destination= {h: A+[destination[h]] for h in H}                     # Unificated set of pickups and destinations in dependence of driver h    \n",
    "    delivery_origin={h: [origin[h]]+A_ for h in H}                             # Unificated set of origins and drop-offs in dependence of driver h    \n",
    "    delivery_destination={h: A_+[destination[h]] for h in H}\n",
    "    Vh={(h): [origin[h]]+A+A_+[destination[h]] for h in H }                    # Set of all nodes in dependance of driver h\n",
    "    origin_pickup_delivery={h: [origin[h]]+A+A_ for h in H}\n",
    "    pickup_delivery_destination={h: pickup_delivery+[destination[h]] for h in H}\n",
    "    origin_delivery_destination=A_+tau[1:]+tau_[1:] \n",
    "    E=[(i, j) for i in V for j in V if i!=j ]                                  # All edges except edges from one node to the same node back\n",
    "    E1={h:[(i,j) for i in Vh[h] for j in Vh[h] if i!=j] for h in H}\n",
    "    d={(i,j):haversine((AllX[i],AllY[i]),(AllX[j],AllY[j])) for i,j in E}      # travel distance for link i-->j\n",
    "    d2={i:haversine((AllX[i],AllY[i]),(AllX[i+n],AllY[i+n])) for i in A}     \n",
    "    d21={i:0 for i in origin_delivery_destination}\n",
    "    d2.update(d21)\n",
    "    t={(i,j):Speed*d[i,j] for i,j in E}\n",
    "    \n",
    "    \n",
    "    return WTP_Faktor, ETP_Faktor, H, R, tau, tau_, V, pickup_delivery, ZF, TW_LB, TW_UB, destination, origin, pickup_destination, delivery_origin, delivery_destination, Vh, origin_pickup_delivery, pickup_delivery_destination, E, E1, d, d2, t, m, n, A, A_, a, b, l, s, R2, AllX, AllY, Speed, district_names, district_coordx, district_coordy \n",
    "all_data=read_data_create_sets(data1)\n",
    "WTP_Faktor, ETP_Faktor, H, R, tau, tau_, V, pickup_delivery, ZF, TW_LB, TW_UB, destination, origin, pickup_destination, delivery_origin, delivery_destination, Vh, origin_pickup_delivery, pickup_delivery_destination, E, E1, d, d2, t, m, n, A, A_, a, b, l, s, R2, AllX, AllY, Speed, district_names, district_coordx, district_coordy  = all_data\n",
    "\n",
    "# Machine learning for the WTP and ETP values\n",
    "def create_wtp_etp():\n",
    "    DP = 150                                                                   # Number of data points                          \n",
    "    maxWTP=10                                                                  # Upper limit for the WTP corridor\n",
    "    minWTP=5                                                                   # Lower limit for the WTP corridor\n",
    "    wtp=dict()\n",
    "    innerstädtisch=[]\n",
    "    außerhalb=[]\n",
    "    distance = {i:d2[i] for i in A}\n",
    "    for i in A:                                                                # Differentiation between the inner and outer areas of Berlin's city center for the pick-up nodes\n",
    "        if 13.32 <= AllX[i] <= 13.47 and 52.48 <= AllY[i] <= 52.525:\n",
    "            base = int((maxWTP-minWTP)/2+minWTP)                               # Calculation of the base value in the inner area\n",
    "            innerstädtisch.append(i)\n",
    "        else:\n",
    "            base = round((maxWTP-minWTP)/2+minWTP,0)                           # Calculation of the base value in the outer range\n",
    "            außerhalb.append(i)\n",
    "        for _ in range(DP):                                                    # Simulating the data points with the WTP variations\n",
    "            wtp_value = base  + np.random.normal(0.2, 0.3)\n",
    "            while wtp_value < minWTP or wtp_value > maxWTP:                    # Compliance with the WTP corridor\n",
    "                wtp_value = base +  np.random.normal(0.1, 0.2)\n",
    "            if i in wtp:\n",
    "                wtp[i].append(wtp_value)\n",
    "            else:\n",
    "                wtp[i] = [wtp_value]\n",
    "    distance_list = []\n",
    "    wtp_list = []\n",
    "    customer_list = []\n",
    "    for customer_id, i in enumerate(A):                                        # Sorting of data points by customer number\n",
    "        for dp in range(DP):\n",
    "            distance_list.append(distance[i])\n",
    "            wtp_list.append(wtp[i][dp])\n",
    "            customer_list.append(i)\n",
    "    # 'Historical data' created\n",
    "   # Create the DataFrame\n",
    "    dfML_WTP = pd.DataFrame({                                                  # Creation of the data frame with the customer numbers, the order distances and the simulated WTP values\n",
    "        'customer_id': customer_list,\n",
    "        'distance': distance_list,\n",
    "        'WTP': wtp_list\n",
    "        })\n",
    "    # Split data into training data, test data and validation data\n",
    "    train_data_WTP, test_data_WTP = train_test_split(dfML_WTP, test_size=0.2, stratify=dfML_WTP['customer_id'], random_state=42)\n",
    "    train_data_WTP, val_data_WTP = train_test_split(train_data_WTP, test_size=0.2, stratify=train_data_WTP['customer_id'], random_state=42)\n",
    "    test_data_WTP_avg = test_data_WTP.groupby('customer_id').mean().reset_index()\n",
    "    # Conversion to Tensors\n",
    "    train_tensors = {}\n",
    "    unique_customers = train_data_WTP['customer_id'].unique()\n",
    "    for customer_id in unique_customers:\n",
    "        sub_data = train_data_WTP[train_data_WTP['customer_id'] == customer_id]\n",
    "        train_distance = torch.tensor(sub_data['distance'].values, dtype=torch.float32)\n",
    "        train_wtp = torch.tensor(sub_data['WTP'].values, dtype=torch.float32)\n",
    "        train_tensors[customer_id] = {\n",
    "            'distance': train_distance,\n",
    "            'wtp': train_wtp\n",
    "            }\n",
    "    SteigungWTP=maxWTP/max(distance_list)                                      # Step length for the beta parameter \n",
    "    def Bayesian_model1(train_distance, train_wtp):                            # Creation of the Bayesian model\n",
    "        # Prior distribution\n",
    "        alpha = pyro.sample(\"alpha\", dist.Normal(0.0, 0.01))\n",
    "        beta_distance = pyro.sample(\"beta_distance\", dist.Normal(SteigungWTP, 0.01))\n",
    "        sigma = pyro.sample(\"sigma\", dist.HalfNormal(0.01))\n",
    "        # Likelihood\n",
    "        mu = alpha + beta_distance * train_distance \n",
    "        with pyro.plate(\"data\", len(train_wtp)):\n",
    "            pyro.sample(\"obs\", dist.Normal(mu, sigma), obs=train_wtp)\n",
    "    posterior_samples_dict = {}\n",
    "    predictions_dict = {}\n",
    "    # Create Predictive-Object\n",
    "    guide = AutoDiagonalNormal(Bayesian_model1)\n",
    "     # Training the model and making predictions for each customer\n",
    "    for customer_id, tensors in train_tensors.items():\n",
    "        train_distance = tensors['distance']\n",
    "        train_wtp = tensors['wtp']\n",
    "        # Bayesian inference\n",
    "        nuts_kernel = NUTS(Bayesian_model1)\n",
    "        mcmc = MCMC(nuts_kernel, num_samples=150, warmup_steps=150)            # Number of samples and warm-up steps for the accuracy of the prediction of the A posteriori distribution\n",
    "        mcmc.run(train_distance, train_wtp)                                    # Markov-Monte-Carlo method to draw samples for the A posteriori distribution\n",
    "        # Posterior Samples\n",
    "        posterior_samples = mcmc.get_samples()\n",
    "        posterior_samples_dict[customer_id] = posterior_samples\n",
    "        def predictive_model(train_distance, alpha=posterior_samples[\"alpha\"], beta_distance=posterior_samples[\"beta_distance\"], sigma=posterior_samples[\"sigma\"]):\n",
    "            mu = alpha + beta_distance * train_distance                        # A posteriori distribution is used for the prediction\n",
    "            return pyro.sample(\"WTP\", dist.Normal(mu, sigma))\n",
    "        # Prediction\n",
    "        predictive1 = Predictive(predictive_model, posterior_samples=posterior_samples, num_samples=150)\n",
    "        new_distance1 = torch.tensor(test_data_WTP_avg[test_data_WTP_avg['customer_id'] == customer_id]['distance'].values, dtype=torch.float32)\n",
    "        samples = predictive1(new_distance1)\n",
    "        predictions_dict[customer_id] = samples\n",
    "    # Preparation Output of WTP mean value in readable format for the pricing model\n",
    "    wtp_mean_dict = {}\n",
    "    wtp_std_dict = {}\n",
    "    for customer_id, samples in predictions_dict.items():\n",
    "        wtp_mean = samples[\"WTP\"].mean(dim=0)\n",
    "        wtp_std = samples[\"WTP\"].std(dim=0)\n",
    "        wtp_mean_dict[customer_id] = wtp_mean\n",
    "        wtp_std_dict[customer_id] = wtp_std\n",
    "    WTP={}\n",
    "    for i, wtp_mean_values in wtp_mean_dict.items():\n",
    "        WTP[i] = float(wtp_mean_values.mean().item())\n",
    "                                        #ETP\n",
    "    maxETP=1                                                                   # Upper limit for the ETP corridor\n",
    "    minETP=0.5                                                                 # Lower limit for the ETP corridor\n",
    "    etp={h:[] for h in H}\n",
    "    innerstädtisch2=[]\n",
    "    außerhalb2=[]\n",
    "    for h in H:\n",
    "        if 13.32 <= AllX[h + 2*n] <= 13.47 and 52.48 <= AllY[h + 2*n] <= 52.525:# Differentiation between the inner and outer areas of Berlin's city center for the pick-up nodes\n",
    "            innerstädtisch2.append(h)\n",
    "            base = int(((maxETP-minETP)/2+minETP)*10)/10                       # Calculation of the base value in the inner area\n",
    "        else:\n",
    "            außerhalb2.append(h)\n",
    "            base = round((maxETP-minETP)/2+minETP,1)                           # Calculation of the base value in the outer area\n",
    "        for _ in range(DP):                                                    # Simulating the data points with the ETP variations\n",
    "            etp_value = base + np.random.normal(0.1, 0.2)\n",
    "            while etp_value < minETP or etp_value > maxETP:                    #  Compliance with the ETP corridor\n",
    "                etp_value = (base + np.random.normal(0.1, 0.2))\n",
    "            if h in etp:\n",
    "                etp[h].append(etp_value)\n",
    "            else:\n",
    "                etp[h] = [etp_value]\n",
    "    # Datensatz distance3 von den Startknoten zu den Zielknoten erstellen                                \n",
    "    distance_driver={h:d[origin[h],destination[h]]for h in H}\n",
    "    distance_list3 = []\n",
    "    etp_list = []\n",
    "    driver_list = []\n",
    "    for driver_id, h in enumerate(H):                                          # Sorting the data points by driver number \n",
    "        for dp in range(DP):\n",
    "            distance_list3.append(distance_driver[h])\n",
    "            etp_list.append(etp[h][dp])\n",
    "            driver_list.append(h) \n",
    "    dfML_ETP = pd.DataFrame({                                                  # Creation of the data frame with the driver numbers, the route distances from start to finish nodes and the simulated ETP values\n",
    "        'driver_id': driver_list,\n",
    "        'distance3': distance_list3,\n",
    "        'ETP': etp_list\n",
    "        })\n",
    "    # Split data into training data, test data and validation data\n",
    "    train_data_ETP, test_data_ETP = train_test_split(dfML_ETP, test_size=0.2, stratify=dfML_ETP['driver_id'], random_state=42)\n",
    "    train_data_ETP, val_data_ETP = train_test_split(train_data_ETP, test_size=0.2,stratify=train_data_ETP['driver_id'], random_state=42)\n",
    "    test_data_ETP_avg = test_data_ETP.groupby('driver_id').mean().reset_index()\n",
    "    # Conversion to tensors\n",
    "    train_tensors2 = {}\n",
    "    unique_drivers = train_data_ETP['driver_id'].unique()\n",
    "    for driver_id in unique_drivers:\n",
    "        sub_data2 = train_data_ETP[train_data_ETP['driver_id'] == driver_id]\n",
    "        train_distance3 = torch.tensor(sub_data2['distance3'].values, dtype=torch.float32)\n",
    "        train_etp = torch.tensor(sub_data2['ETP'].values, dtype=torch.float32)\n",
    "        train_tensors2[driver_id] = {\n",
    "            'distance3': train_distance3,\n",
    "            'etp': train_etp\n",
    "            }\n",
    "    SteigungETP=float(-(maxETP-minETP)/(max(distance_list3)-min(distance_list3)))  # Step length for the beta parameter  \n",
    "    def Bayesian_model2( train_distance3, train_etp):                          # Creation of the Bayesian model\n",
    "        # Prior Distribution\n",
    "        alpha2 = pyro.sample(\"alpha\", dist.Normal(maxETP, 0.001))\n",
    "        beta_distance3 = pyro.sample(\"beta_distance3\", dist.Normal(SteigungETP, 0.001))\n",
    "        sigma2 = pyro.sample(\"sigma\", dist.HalfNormal(0.001))\n",
    "        # Likelihood\n",
    "        mu2 = alpha2 + beta_distance3 * train_distance3 \n",
    "        with pyro.plate(\"data\", len(train_etp)):\n",
    "            pyro.sample(\"obs\", dist.Normal(mu2, sigma2), obs=train_etp)\n",
    "    posterior_samples_dict2 = {}\n",
    "    predictions_dict2 = {}\n",
    "    guide = AutoDiagonalNormal(Bayesian_model2)\n",
    "    for driver_id, tensors2 in train_tensors2.items():\n",
    "        train_distance3 = tensors2['distance3']\n",
    "        train_etp = tensors2['etp']\n",
    "        # Bayessche Inferez\n",
    "        nuts_kernel = NUTS(Bayesian_model2)\n",
    "        mcmc2 = MCMC(nuts_kernel, num_samples=150, warmup_steps=150)           # Number of samples and warm-up steps for the accuracy of the prediction of the A posteriori distribution\n",
    "        mcmc2.run(train_distance3, train_etp)                                  # Markov-Monte-Carlo method to draw samples for the A posteriori distribution\n",
    "        # Posterior Samples\n",
    "        posterior_samples2 = mcmc2.get_samples()\n",
    "        posterior_samples_dict2[driver_id] = posterior_samples2\n",
    "        def predictive_model2(train_distance3, alpha2=posterior_samples2[\"alpha\"], beta_distance3=posterior_samples2[\"beta_distance3\"], sigma2=posterior_samples2[\"sigma\"] ):\n",
    "            mu2 = alpha2 + beta_distance3 * train_distance3                    # A posteriori distribution is used for the prediction\n",
    "            return pyro.sample(\"ETP\", dist.Normal(mu2, sigma2))\n",
    "        # Prediction\n",
    "        predictive2 = Predictive(predictive_model2, posterior_samples=posterior_samples2, num_samples=150)\n",
    "        new_distance3 = torch.tensor(test_data_ETP_avg[test_data_ETP_avg['driver_id'] == driver_id]['distance3'].values, dtype=torch.float32)\n",
    "        samples2 = predictive2(new_distance3)\n",
    "        predictions_dict2[driver_id] = samples2\n",
    "    # Preparation of the output of the ETP averages\n",
    "    etp_mean_dict={}\n",
    "    etp_std_dict={}\n",
    "    for driver_id, samples2 in predictions_dict2.items():\n",
    "        etp_mean = samples2[\"ETP\"].mean(dim=0)\n",
    "        etp_std = samples2[\"ETP\"].std(dim=0)\n",
    "        etp_mean_dict[driver_id] = etp_mean\n",
    "        etp_std_dict[driver_id] = etp_std\n",
    "    # Output of ETP mean values in readable format for the pricing model\n",
    "    ETP={}\n",
    "    for h, etp_mean_value in etp_mean_dict.items():\n",
    "        ETP[h] = float(etp_mean_value.mean().item())\n",
    "    return WTP, wtp_list, dfML_WTP, ETP,  etp_list,  dfML_ETP\n",
    "ML=create_wtp_etp()\n",
    "WTP, wtp_list, dfML_WTP, ETP, etp_list, dfML_ETP=ML\n",
    "\n",
    "# Running Optimization\n",
    "result = pricing_model(n, m, A, A_, H, WTP, ETP, R, d2, V, d, t, s, l, a, b, M)# running the function for optimization model\n",
    "if result is not None:                                                         # print results when optimization successful  \n",
    "    OF_p, optimal_L, optimal_S, optimal_c, optimal_p, optimal_x, optimal_z, Runtime1 = result      \n",
    "else:\n",
    "    print(\"Optimization failed.\")\n",
    "\n",
    "    \n",
    "#extract variables\n",
    "def results():\n",
    "    active_arcs=[(i, j, h) for i, j in E for h in H if optimal_x[i, j, h]>0.99] \n",
    "    active_prices=[(i, j, h) for i, j in E for h in H if optimal_p[i, j, h]>0.99]\n",
    "    active_costs=[(i, j, h) for i, j in E for h in H if optimal_c[i, j, h]>0.099]   \n",
    "    active_load=[(i, h) for i in V for h in H  if optimal_L[i, h]>0.99] \n",
    "    active_z=[i for i in A if optimal_z[i]>0.99] \n",
    "    active_Start=[(i, h) for i in V for h in H if optimal_S[i, h]>0.99]\n",
    "    td=sum(optimal_x[i, j, h]*d[i,j] for h in H for i in origin_pickup_delivery[h] for j in pickup_delivery  if i!=j)\n",
    "    Fahrerstopps={h:sum(optimal_x[i,j,h] for i,j in E) for h in H}\n",
    "    Multistop_touren={h:Fahrerstopps[h] for h in H}\n",
    "    Touren={h:[(i,j) for i,j in E if optimal_x[i,j,h]>0.99 ] for h in H}\n",
    "    for h in H:\n",
    "        if Fahrerstopps[h]>=2:\n",
    "            Multistop_touren[h]=(Fahrerstopps[h]-1)/2\n",
    "        else:\n",
    "            del Multistop_touren[h]\n",
    "            del Touren[h]\n",
    "    active_driver=[h for h in Touren if Touren[h]!=[] ]\n",
    "    possible_matches= len(A)\n",
    "    matches=len(A)-sum(optimal_z[i] for i in A)\n",
    "    used_driver=sum(optimal_x[i,j,h] for i in A_ for j in tau_[1:] for h in H)\n",
    "    active_arcs2D=list()\n",
    "    for a in active_arcs:\n",
    "        active_arcs2D.extend([a[:2]])\n",
    "    active=list()\n",
    "    for (i,j) in active_arcs2D:\n",
    "        if i in tau[1:] and j in tau_[1:]:\n",
    "            pass\n",
    "        else:\n",
    "            active.append((i,j))\n",
    "    chosen_c={(i,j,h):optimal_c[i, j, h] for i,j in E for h in H if optimal_c[i, j, h]>0.099}\n",
    "    chosen_cost={h:chosen_c[i,j,h] for i,j,h in chosen_c}\n",
    "    chosen_p={(i):round(optimal_p[i, j, h],2) for i,j in E for h in H if optimal_p[i, j, h]>0.099}\n",
    "    opt_request_p={i:round(chosen_p[i]*d2[i],2) for i in chosen_p}\n",
    "    opt_driver_c={(h):round(chosen_c[i,j,h],2) for i,j,h in chosen_c}\n",
    "    Einnahmen=round(sum(optimal_p[i,j,h]*d2[i] for i,j in E for h in H),2)\n",
    "    Einnahmen_je_Tour={h:round(sum(optimal_p[i,j,h]*d2[i] for i,j in E ),2)for h in H}\n",
    "    Ausgaben=round(sum(optimal_c[i,j,h]*d[i,j] for i,j in E for h in H),2)\n",
    "    Ausgaben_je_Tour={h:round(sum(optimal_c[i,j,h]*d[i,j] for i,j in E ),2)for h in H}\n",
    "    Marge_je_Tour={h:Einnahmen_je_Tour[h]-Ausgaben_je_Tour[h] for h in active_driver}\n",
    "    aktive_Auftraege=[i for i in chosen_p]\n",
    "    Einnahmen_je_Auftrag={i:d2[i]*chosen_p[i] for i in aktive_Auftraege}\n",
    "    Fahrerdistanz_je_Auftrag={i:d[i,j]+d[a,i] for i in aktive_Auftraege for j in pickup_delivery for h in H for a in origin_pickup_delivery[h] if (i,j) in active and (a,i) in active }\t\n",
    "    Ausgaben_je_Auftrag={i:Fahrerdistanz_je_Auftrag[i]*chosen_c[i,j,h] for i,j,h in chosen_c if i in aktive_Auftraege}\n",
    "    Marge_je_Auftrag={i:(Einnahmen_je_Auftrag[i]-Ausgaben_je_Auftrag[i]) for i in chosen_p} \n",
    "    Kantenkosten={(i,j):chosen_c[i,j,h] for h in active_driver for (i,j) in Touren[h] if j not in tau_[1:]}\n",
    "    print(\"total distance from origin to dropoff=\", td)\n",
    "    print(\"active_arcs (x):\", active_arcs)\n",
    "    print(\"active_z (z):\", active_z)\n",
    "    print(\"Anzahl Aufträge je Fahrer:\", Multistop_touren)\n",
    "    print(\"Anzahl möglicher matches=\",possible_matches)\n",
    "    print(\"Anzahl erfolgreicher matches=\", matches)\n",
    "    print(\"Anzahl genutzter Fahrer:\", used_driver)\n",
    "    print(\"Folgende Fahrer werden eingesetzt:\", active_driver)\n",
    "    print(\"ZFW (Marge Platform Provider in €):\", round(OF_p,2))\n",
    "    print(\"Marge je Auftrag:\", [Marge_je_Auftrag])\n",
    "    for i in chosen_p:\n",
    "        print(\"WTP bei Auftrag\", [i], \"=\", WTP[i], \"gewählter Preis für Auftrag\", [i], \"=\", opt_request_p[i], \"gewählter Preis je km =\", round(chosen_p[i],2) )\n",
    "    for h in active_driver:\n",
    "        print(\"ETP for driver\", [h], \"=\", ETP[h], \"choosen cost per km =\", opt_driver_c[h] )\n",
    "    print(\"Einnahmen=\", Einnahmen, \"Ausgaben=\", Ausgaben)\n",
    "    return active_arcs, active_prices, active_costs, active_load, active_z, active_Start, Touren, active_driver, active_arcs2D, active, chosen_c, chosen_cost, chosen_p, opt_request_p, opt_driver_c, Einnahmen, Ausgaben, used_driver, possible_matches, matches, Kantenkosten, Marge_je_Auftrag, Marge_je_Tour, aktive_Auftraege\n",
    "result2 = results()\n",
    "active_arcs, active_prices, active_costs, active_load, active_z, active_Start, Touren, active_driver, active_arcs2D, active, chosen_c, chosen_cost, chosen_p, opt_request_p, opt_driver_c, Einnahmen, Ausgaben, used_driver, possible_matches, matches, Kantenkosten, Marge_je_Auftrag, Marge_je_Tour, aktive_Auftraege = result2\n",
    "\n",
    "def variables_part2():\n",
    "    \n",
    "    active_nodes=[]\n",
    "    for a in active:\n",
    "        active_nodes.extend(a[:1])\n",
    "        active_nodes.extend(a[1:])\n",
    "    active_nodes=list(dict.fromkeys(active_nodes))\n",
    "    Touren_neu={h:(tuple) for h in Touren}\n",
    "    for h in Touren:\n",
    "        edgars=[(i,j) for (i,j) in Touren[h]]\n",
    "        GTouren = nx.DiGraph()\n",
    "        GTouren.add_edges_from(edgars)\n",
    "        longest_path = nx.dag_longest_path(GTouren)\n",
    "        graph_after = list(zip(longest_path[:-1], longest_path[1:]))\n",
    "        Touren_neu[h]=graph_after\n",
    "    Touren_sorted={h:list() for h in Touren_neu}\n",
    "    for h in Touren_neu:\n",
    "        liste=[]\n",
    "        for a in Touren_neu[h]:\n",
    "            list(a)\n",
    "            liste.append(a[:1])\n",
    "        liste.append(a[1:])    \n",
    "        liste=[i[0] for i in liste]\n",
    "        Touren_sorted[h]=liste\n",
    "    opt_S_driver={(i,h):optimal_S[i,h]  for h in Touren_sorted for i in  Touren_sorted[h]}\n",
    "    Starting_times={i:int(optimal_S[i,h]) for h in Touren_sorted for i in Touren_sorted[h] }\n",
    "    Starting_times_formatted={}\n",
    "    for key, value in Starting_times.items():\n",
    "        time_obj = timedelta(minutes=value)\n",
    "        time_str = (datetime.min + time_obj).strftime('%H:%M')\n",
    "        Starting_times_formatted[key] = time_str\n",
    "    activitis=list()\n",
    "    for h in H:\n",
    "        if h not in active_driver:\n",
    "            not_driving=h*-1\n",
    "            activitis.append(not_driving)\n",
    "        else:\n",
    "            activitis.append(h)\n",
    "    activitis.insert(0,0)\n",
    "    used_origins={h:origin[h] for h in origin if h in active_driver}\n",
    "    TW_LB_driver={h: str for h in used_origins}\n",
    "    for h,i in used_origins.items():\n",
    "        TW_LB_driver[h]=TW_LB[i]\n",
    "    used_destinations={h:destination[h] for h in destination if h in active_driver}\n",
    "    TW_UB_driver={h: str for h in used_destinations}\n",
    "    for h,i in used_destinations.items():\n",
    "        TW_UB_driver[h]=TW_UB[i]\n",
    "    active_t={i:t[i,j] for i,j in active}\n",
    "    print(\"Tour je Fahrer:\", Touren_sorted)\n",
    "    used_ETP={h:ETP[h] for h in active_driver}\n",
    "    used_WTP={i:WTP[i] for i in active_nodes if i in A}\n",
    "    Auftrag_Fahrer={node:driver for driver in Touren_sorted for node in Touren_sorted[driver]}\n",
    "    Senders_surplus=round(sum(WTP[i] for i in aktive_Auftraege)-Einnahmen,2) \n",
    "    Couriers_surplus=round(Ausgaben-sum(ETP[h]*d[i,j]*optimal_x[i,j,h] for i,j in E for h in H if i in origin_pickup_delivery[h] and j in pickup_delivery),2)\n",
    "    Umweg={(h): d[Touren_sorted[h][-2],destination[h]] for h in Touren_sorted }\n",
    "    Umwege_summe=round(sum(Umweg[h]for h in Umweg),2)\n",
    "    ges_km_je_Tour= {h: round(sum(d[i] for i in Touren_neu[h]),2) for h in Touren_neu}\n",
    "    origin_dest_km={h: round(d[origin[h],destination[h]],2) for h in H}\n",
    "    Umweg_ges_km=round(Umwege_summe/sum(ges_km_je_Tour[h] for h in Touren_sorted ),2)\n",
    "    Umweg_Tour_Verhältnis={h:round(Umweg[h]/ges_km_je_Tour[h],2) for h in Touren_sorted}\n",
    "    km_Auftrag={i:d[i,i+n] for i in A}\n",
    "    Umweg_je_Tour={h: Umweg[h]/origin_dest_km[h] for h in Touren_sorted}\n",
    "    ave_Umweg_je_Tour=np.mean(list(Umweg_je_Tour.values()))\n",
    "    origin_dest_km2={h: round(d[origin[h],destination[h]],2) for h in H if h not in Touren_neu}  \n",
    "    gefahrene_km=sum(ges_km_je_Tour[h] for h in ges_km_je_Tour)+sum(origin_dest_km2[h] for h in origin_dest_km2)\n",
    "    Externalitaet=gefahrene_km/sum(origin_dest_km[h] for h in H)\n",
    "    return Touren_neu, Touren_sorted,used_origins,TW_LB_driver,TW_UB_driver, used_destinations, activitis, opt_S_driver,  Touren, active_nodes, Starting_times, Starting_times_formatted, active_t, used_ETP, used_WTP, Auftrag_Fahrer, Senders_surplus, Couriers_surplus, Umwege_summe, Umweg, Umweg_ges_km, ges_km_je_Tour, origin_dest_km, Umweg_Tour_Verhältnis, km_Auftrag, Umweg_je_Tour, ave_Umweg_je_Tour, origin_dest_km2, gefahrene_km, Externalitaet\n",
    "result3 = variables_part2()\n",
    "Touren_neu, Touren_sorted,used_origins,TW_LB_driver,TW_UB_driver, used_destinations, activitis, opt_S_driver,  Touren, active_nodes, Starting_times, Starting_times_formatted, active_t, used_ETP, used_WTP, Auftrag_Fahrer, Senders_surplus, Couriers_surplus, Umwege_summe, Umweg, Umweg_ges_km, ges_km_je_Tour, origin_dest_km, Umweg_Tour_Verhältnis, km_Auftrag, Umweg_je_Tour, ave_Umweg_je_Tour, origin_dest_km2, gefahrene_km, Externalitaet = result3 \n",
    "\n",
    "\n",
    "#draw graphs\n",
    "def graph_map():\n",
    "    file_path = os.path.join(os.getcwd(), \"data_Berlin\", \"berlin_ortsteile.shp\")\n",
    "    districts = gpd.read_file(file_path)\n",
    "    G=nx.DiGraph()\n",
    "    G.add_nodes_from(V)\n",
    "    G.add_edges_from(active)\n",
    "    pos={i: (AllX[i],AllY[i])for i in V}\n",
    "    fig1 = plt.figure(figsize=(16, 10))\n",
    "    graph_ax = fig1.add_subplot(1, 1, 1,projection=ccrs.PlateCarree())\n",
    "    graph_ax=plt.axes([0,0,1,1], projection=ccrs.PlateCarree())\n",
    "    graph_ax.add_feature(cfeature.LAKES, edgecolor='black', alpha=0.5)\n",
    "    graph_ax.add_feature(cfeature.RIVERS, edgecolor='black', alpha=0.9)\n",
    "    for i in district_names:\n",
    "        graph_ax.text(district_coordy[i], district_coordx[i], district_names[i], fontsize=8, color='grey', ha='center', va='top')\n",
    "    districts.boundary.plot(ax=graph_ax, color='gray',  linewidth=1, alpha=0.5)\n",
    "    graph_ax.add_feature(cfeature.NaturalEarthFeature(category='cultural', name='admin_0_boundary_lines_land', scale='10m', facecolor=None))\n",
    "    graph_ax.set_extent([13.27, 13.51, 52.450, 52.56], crs=ccrs.PlateCarree())\n",
    "    file_path2 = os.path.join(os.getcwd(), \"data_Berlin\", \"Umweltzone_Berlin.shp\")\n",
    "    Umweltzone = gpd.read_file(file_path2)\n",
    "    Umweltzone = Umweltzone.set_crs(\"EPSG:25833\")\n",
    "    target_crs = ccrs.PlateCarree()    \n",
    "    Umweltzone = Umweltzone.to_crs(target_crs.proj4_init)\n",
    "    for idx, polygon in Umweltzone.iterrows():\n",
    "        graph_ax.add_geometries([polygon['geometry']], crs=ccrs.PlateCarree(), facecolor='green', edgecolor='black', alpha=0.1)\n",
    "    options = {\"edgecolors\": \"k\", \"node_size\": 200, \"alpha\": 0.5}\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=tau[1:], node_color=\"yellow\", node_shape='H', **options)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=A, node_color=\"tab:orange\", node_shape='o', **options)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=A_, node_color=\"tab:red\", node_shape='o', **options)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=tau_[1:], node_color=\"tab:brown\", node_shape='d', **options)\n",
    "    nx.draw_networkx_labels(G, pos, labels={n:n for n in G}, font_size=8, font_color='k' )\n",
    "    edges=list(G.edges())\n",
    "    num_edges=len(edges)+1\n",
    "    colormap= plt.colormaps['tab20']\n",
    "    colores=[colormap(i/(num_edges)) for i in range(max(Touren)+1)]\n",
    "    edgecolors={(a):colores[h] for h in Touren for (a) in Touren[h]}\n",
    "    driver_color={(h):colores[h] for h in active_driver}\n",
    "    edge_color_list = [edgecolors[a] for a in edges]\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=edge_color_list, alpha=1.0 ,width=1.5, arrows=True, arrowsize=12, ax=graph_ax, connectionstyle=\"arc3,rad=0.1\")\n",
    "    graph_ax.set_xticks([])\n",
    "    graph_ax.set_yticks([])\n",
    "    graph_ax.set_xlim(13.27, 13.51)\n",
    "    graph_ax.set_ylim(52.450, 52.562)\n",
    "    graph_ax.gridlines(draw_labels=True)\n",
    "    graph_ax.add_feature(cfeature.LAND)\n",
    "    plt.title(\"Tourenmap:IPIC_SP_p\")\n",
    "    plt.show()\n",
    "    \n",
    "    return edgecolors, colores, num_edges, driver_color, fig1\n",
    "result4=graph_map()\n",
    "edgecolors, colores, num_edges, driver_color, fig1 = result4\n",
    "\n",
    "\n",
    "def graph_gantt():    \n",
    "    Ending_times = {}\n",
    "    Ending_times = {i: (Starting_times[i] + s[i] + active_t[i]) for i in Starting_times if i not in tau_[1:]}\n",
    "    Ending_times2 = {i: Starting_times[i] for i in tau_[1:] if i in active_nodes}\n",
    "    Ending_times.update(Ending_times2)\n",
    "    Ending_times_formatted = {i: 0 for i in Starting_times}\n",
    "    for key, value in Ending_times.items():\n",
    "        time_obj = timedelta(minutes=value)\n",
    "        time_str = (datetime.min + time_obj).strftime('%H:%M')\n",
    "        Ending_times_formatted[key] = time_str\n",
    "    start_3={i:Starting_times[i] + s[i]  for i in Starting_times if i not in tau_[1:]}\n",
    "    start_32 = {i: Starting_times[i] for i in tau_[1:] if i in active_nodes}\n",
    "    start_3.update(start_32)\n",
    "    start3= {i: 0 for i in Starting_times}\n",
    "    for key, value in start_3.items():\n",
    "        time_obj = timedelta(minutes=value)\n",
    "        time_str = (datetime.min + time_obj).strftime('%H:%M')\n",
    "        start3[key] = time_str\n",
    "    end_3={i:Starting_times[i] + s[i] + active_t[i]  for i in Starting_times if i not in tau_[1:]}\n",
    "    end_32={i: Starting_times[i] for i in tau_[1:] if i in active_nodes}\n",
    "    end_3.update(end_32)\n",
    "    end3 = {i: 0 for i in Starting_times}\n",
    "    for key, value in end_3.items():\n",
    "        time_obj = timedelta(minutes=value)\n",
    "        time_str = (datetime.min + time_obj).strftime('%H:%M')\n",
    "        end3[key] = time_str\n",
    "    df = []\n",
    "    for h, i in Touren_sorted.items():\n",
    "       for j in i:\n",
    "            df.append({'task': j, 'Start': Starting_times_formatted[j], 'Finish': Ending_times_formatted[j], 'Resource': h, 'color': driver_color[h], 'start2':TW_LB_driver[h], 'end2':TW_UB_driver[h], 'start3':start3[j], 'end3':end3[j]})\n",
    "    fig2, gantt_ax = plt.subplots(figsize=(16, 8))\n",
    "    for item in df:\n",
    "        start = datetime.strptime(item['Start'], '%H:%M')\n",
    "        finish = datetime.strptime(item['Finish'], '%H:%M')\n",
    "        duration = finish - start\n",
    "        text_x2 = start+ duration/5\n",
    "        text_y2 = item['Resource']\n",
    "        gantt_ax.broken_barh([(start, duration)], (item['Resource'] - 0.25, 0.5), facecolors=item['color'], zorder=3, alpha=0.5)\n",
    "        gantt_ax.annotate(item['task'], (text_x2, text_y2), xytext=(0, 0), textcoords='offset points',\n",
    "                          ha='center', va='bottom', fontsize=9, color='black', zorder=5)\n",
    "        start2 = datetime.strptime(item['start2'], '%H:%M')\n",
    "        end2 = datetime.strptime(item['end2'], '%H:%M')\n",
    "        start3= datetime.strptime(item['start3'], '%H:%M')\n",
    "        end3 = datetime.strptime(item['end3'], '%H:%M')\n",
    "        duration2 = end2 - start2\n",
    "        duration3 = end3 - start3\n",
    "        gantt_ax.broken_barh([(start2, duration2)], (item['Resource'] -0.25, 0.5), facecolors=item['color'], zorder=2, alpha=0.1)\n",
    "        gantt_ax.broken_barh([(start3, duration3)], (item['Resource'] - 0.25, 0.5), facecolor='none', edgecolor='lightgrey', hatch='//', zorder=4)\n",
    "    Tourdauer = mpatches.Patch(color='tab:red', label='Servicezeit am Knoten')\n",
    "    Fahrer_ZF = mpatches.Patch(color='pink', label='Zeitfenster der Fahrer in hell')\n",
    "    Fahrtzeit = mpatches.Patch(facecolor='none', hatch='//', edgecolor='grey', label='Fahrtzeit')\n",
    "    Knoten_nummer = mpatches.Patch(color='white', label='1,2,.: Nr besuchter Knoten ')\n",
    "    gantt_ax.legend(handles=[Tourdauer, Fahrer_ZF, Fahrtzeit, Knoten_nummer], loc='lower right')    \n",
    "    plt.xlabel(\"Zeit\")\n",
    "    plt.ylabel(\"Fahrer\")\n",
    "    plt.title(\"Gantt:IPIC_SP_p\")\n",
    "    y_ticks = list(Touren_sorted.keys())\n",
    "    y_labels = list(Touren_sorted.keys())  \n",
    "    y_pos=list(Touren_sorted.keys())\n",
    "    gantt_ax.set_yticks(y_pos, y_ticks)\n",
    "    gantt_ax.set_yticklabels(y_labels) \n",
    "    x_ticks = pd.date_range(start=datetime(1900, 1, 1, 10, 0), end=datetime(1900, 1, 1, 20, 00), freq='h')\n",
    "    x_labels = [time.strftime('%H:%M') for time in x_ticks]\n",
    "    gantt_ax.set_xticks(x_ticks)\n",
    "    gantt_ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "    gantt_ax.grid(zorder=0)\n",
    "    plt.show()\n",
    "    return Ending_times, Ending_times_formatted, df, start3, fig2\n",
    "\n",
    "result5 = graph_gantt()\n",
    "Ending_times, Ending_times_formatted, df, start3, fig2 =result5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
